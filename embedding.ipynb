{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5EncoderModel\n",
    "import torch\n",
    "import re\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 01. Environment Settings\n",
    "Set the device to GPU (if available) or CPU so that the code knows where to perform the model's calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.1+cu121\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "True\n",
      "1\n",
      "NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "print(device)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 02. Load Tokenizer and model:\n",
    "The pre-trained Tokenizer and T5 encoder models were loaded using the from_pretrained method and the models were moved to the previously specified device (GPU or CPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained('Rostlab/prot_t5_xl_half_uniref50-enc', do_lower_case=False)\n",
    "model = T5EncoderModel.from_pretrained(\"Rostlab/prot_t5_xl_half_uniref50-enc\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 03. Preparing and Processing Protein Sequences\n",
    "Protein sequences were first defined, and then rare or indeterminate amino acids were replaced with X's by regular expression substitutions and the addition of spaces, and each amino acid in the sequence was correctly separated for the model input format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('epitope_cdr3_pair.csv', 'r') as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    \n",
    "    sequence_examples = []\n",
    "    next(csv_reader)\n",
    "    for row in csv_reader:\n",
    "        \n",
    "        sequence_examples.append(row[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_examples = [\" \".join(list(re.sub(r\"[UZOB]\", \"X\", sequence))) for sequence in sequence_examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = tokenizer(sequence_examples, add_special_tokens=True, padding=\"longest\")\n",
    "input_ids = torch.tensor(ids['input_ids']).to(device)\n",
    "attention_mask = torch.tensor(ids['attention_mask']).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    embedding_repr = model(input_ids=input_ids, attention_mask=attention_mask)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_0 = embedding_repr.last_hidden_state[0,:13]\n",
    "emb_1 = embedding_repr.last_hidden_state[1,:14]\n",
    "emb_0_per_protein = emb_0.mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.1421,  0.0178, -0.1057,  ...,  0.1298,  0.0049,  0.0661],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(emb_0_per_protein)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_0_per_protein.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_1_per_protein = emb_1.mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.1518,  0.1074, -0.0823,  ...,  0.1613, -0.0450, -0.0311],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(emb_1_per_protein)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
